{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "jupyter: python3\n",
        "from: markdown+emoji\n",
        "execute:\n",
        "  echo: true\n",
        "  eval: true\n",
        "  message: false\n",
        "  warning: false\n",
        "---\n",
        "\n",
        "# Classifications d'images supervisées {#sec-chap05}\n",
        "\n",
        "\n",
        "## :rocket: Préambule\n",
        "\n",
        "Assurez-vous de lire ce préambule avant d'exécutez le reste du notebook.\n",
        "\n",
        "### :dart: Objectifs\n",
        "Dans ce chapitre, nous abordons quelques techniques d'apprentissage automatique. Ce chapitre est aussi disponible sous la forme d'un notebook Python sur Google Colab:\n",
        "\n",
        "[![](images/colab-badge.svg)](https://colab.research.google.com/github/sfoucher/TraitementImagesPythonVol1/blob/main/notebooks/05-ClassificationsSupervisees.ipynb){target=\"_blank\"} \n",
        "\n",
        "### Librairies\n",
        "Les librairies qui vont être explorées dans ce chapitre sont les suivantes:\n",
        "\n",
        "* [SciPy](https://scipy.org/)\n",
        "\n",
        "* [NumPy](https://numpy.org/) \n",
        "\n",
        "* [opencv-python · PyPI](https://pypi.org/project/opencv-python/)\n",
        "\n",
        "* [scikit-image](https://scikit-image.org/)\n",
        "\n",
        "* [Rasterio](https://rasterio.readthedocs.io/en/stable/)\n",
        "\n",
        "* [Xarray](https://docs.xarray.dev/en/stable/)\n",
        "\n",
        "* [rioxarray](https://corteva.github.io/rioxarray/stable/index.html)\n",
        "\n",
        "* [geopandas](https://geopandas.org)\n",
        "\n",
        "* [scikit-learn](https://scikit-learn.org/)\n",
        "\n",
        "Dans l'environnement Google Colab, seul `rioxarray` doit être installés:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: false\n",
        "%%capture\n",
        "!pip install -qU matplotlib rioxarray xrscipy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vérifier les importations:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import rioxarray as rxr\n",
        "from scipy import signal\n",
        "import xarray as xr\n",
        "import rasterio\n",
        "import xrscipy\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import ListedColormap\n",
        "import geopandas\n",
        "from shapely.geometry import Point\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Images utilisées\n",
        "\n",
        "Nous allons utilisez les images suivantes dans ce chapitre:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: false\n",
        "%%capture\n",
        "import gdown\n",
        "\n",
        "gdown.download('https://drive.google.com/uc?export=download&confirm=pbef&id=1a6Ypg0g1Oy4AJt9XWKWfnR12NW1XhNg_', output= 'RGBNIR_of_S2A.tif')\n",
        "gdown.download('https://drive.google.com/uc?export=download&confirm=pbef&id=1a4PQ68Ru8zBphbQ22j0sgJ4D2quw-Wo6', output= 'landsat7.tif')\n",
        "gdown.download('https://drive.google.com/uc?export=download&confirm=pbef&id=1_zwCLN-x7XJcNHJCH6Z8upEdUXtVtvs1', output= 'berkeley.jpg')\n",
        "gdown.download('https://drive.google.com/uc?export=download&confirm=pbef&id=1dM6IVqjba6GHwTLmI7CpX8GP2z5txUq6', output= 'SAR.tif')\n",
        "gdown.download('https://drive.google.com/uc?export=download&confirm=pbef&id=1aAq7crc_LoaLC3kG3HkQ6Fv5JfG0mswg', output= 'carte.tif')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vérifiez que vous êtes capable de les lire :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| output: false\n",
        "\n",
        "with rxr.open_rasterio('berkeley.jpg', mask_and_scale= True) as img_rgb:\n",
        "    print(img_rgb)\n",
        "with rxr.open_rasterio('RGBNIR_of_S2A.tif', mask_and_scale= True) as img_rgbnir:\n",
        "    print(img_rgbnir)\n",
        "with rxr.open_rasterio('SAR.tif', mask_and_scale= True) as img_SAR:\n",
        "    print(img_SAR)\n",
        "with rxr.open_rasterio('carte.tif', mask_and_scale= True) as img_carte:\n",
        "    print(img_carte)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Principes généraux\n",
        "\n",
        "Une classification supervisée consiste à attribuer une étiquette (une classe) de manière automatique à chaque point d'un jeu de données. Cette classification peut se faire à l'aide d'une cascade de règles pré-établies (arbre de décision) ou à l'aide de techniques d'apprentissage automatique (*machine learning*). L'utilisation de règles pré-établies atteint vite une limite car ces règles doivent être fournies manuellement par un expert. Ainsi, l'avantage de l'apprentissage automatique est que les règles de décision sont dérivées automatiquement du jeu de données via une phase dite d’entraînement. Cet ensemble de règles est souvent appelé **modèle**. On visualise souvent ces règles sous la forme de frontières de décisions dans l'espace des données. Par contre, un des défis majeur de ce type de technique est d'être capable de produire des règles qui soient généralisables au-delà du jeu d’entraînement. \n",
        "\n",
        "Les classifications dirigées présupposent donc que nous avons à disposition un jeu d’entraînement déjà étiqueté. Celui-ci va nous permettre de construire un modèle . Afin que ce modèle soit représentatif et robuste, il nous faut assez de données d’entraînement. Les algorithmes d'apprentissage automatique sont très nombreux et plus ou moins complexes pouvant produire des frontières de décision très complexes et non linéaires. \n",
        "\n",
        "**curse of dimensionnality, capacité d'un modèle, sur-aprrentissage, sous-apprentissage**\n",
        "\n",
        "\n",
        "## Méthode des parallélépipèdes {#sec-0511}\n",
        "\n",
        "La méthode du parallélépipède est probablement la plus simple et consiste à délimiter directement le domaine des points d'une classe par une boite (un parallélépipède) à D dimensions. Les limites de ces parallélépipèdes forment alors des frontières de décision manuelles qui vont permettre décider de la classe d'appartenance d'un nouveau point. Un des avantages de cette technique est que si un point n'est dans aucun parallélépipède alors on peut le laisser comme non classifié. Par contre, la construction de ces parallélépipèdes se complexifient grandement avec le nombre de bandes. À une dimension, deux paramètres, équivalents à un seuillage d'histogramme, sont suffisants. À deux dimensions, vous devez définir 4 segments par classe. Avec 3 bandes, vous devez définir 6 plans par classes et à D dimensions, D hyperplans à D-1 dimensions par classe.\n",
        "\n",
        "### La malédiction de la haute dimension\n",
        "\n",
        "Augmenter le nombre de dimension ou de caractéristiques des données permet de résoudre des problèmes complexes comme la classification d'image. Cependant, cela amène beaucoup de contraintes sur le volume des données. Supposons que nous avons N points occupant un segment linéaire de taille d. La densité de points est $N/d$. Si nous augmentons le nombre de dimension D, la densité de points va diminuer exponentiellement en $1/d^D$. Par conséquent, pour garder une densité constante et donc une bonne estimation des parallélépipèdes, il nous faudrait augmenter le nombre de points en puissance de D. Ceci porte le nom de la malédiction de la dimensionnalité (*dimensionality curse*). En résumé, l'espace vide augmente plus rapidement que le nombre de données d'entraînement et l'espace des données devient de plus en plus parcimonieux (*sparse*). Pour contrecarrer ce problème, on peut sélectionner les meilleures caractéristiques ou appliquer une réduction de dimension. \n",
        "\n",
        "## Méthodes paramétriques\n",
        "\n",
        "Les méthodes paramétriques se basent sur des modélisations statistiques des données pour permettre une classification. La classification consiste alors à trouver la classe la plus vraisemblable dont le modèle statistique décrit le mieux les valeurs observées. On parle alors d'approche de type **maximum de vraisemblance**. \n",
        "\n",
        "Le modèle statistique le plus employé est certainement le modèle Gaussien multivarié:\n",
        "\n",
        "\n",
        "L'ensemble d’entraînement permettra alors de calculer les paramètres de chaque Gaussienne pour chacune des classes d'intérêt. \n",
        "\n",
        "### Construction d'un ensemble d’entraînement\n",
        "\n",
        "Une façon simple de construire un ensemble d’entraînement est d'échantillonner un produit existant. Nous allons utiliser la carte d'occupation des cols suivante qui contient 12 classes différentes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "couleurs_classes= {'NoData': 'black', 'Commercial': 'yellow', 'Nuages': 'lightgrey', \n",
        "                    'Foret': 'darkgreen', 'Faible_végétation': 'green', 'Sol_nu': 'saddlebrown',\n",
        "                  'Roche': 'dimgray', 'Route': 'red', 'Urbain': 'orange', 'Eau': 'blue', 'Tourbe': 'salmon', 'Végétation éparse': 'darkgoldenrod', 'Roche avec végétation': 'darkseagreen'}\n",
        "nom_classes= [*couleurs_classes.keys()]\n",
        "couleurs_classes= [*couleurs_classes.values()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import rioxarray as rxr\n",
        "cmap_classes = ListedColormap(couleurs_classes)\n",
        "\n",
        "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(8, 6))\n",
        "img_carte.squeeze().plot.imshow(cmap=cmap_classes, vmin=0, vmax=12)\n",
        "ax.set_title(\"Carte d'occupation des sols\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "On peut facilement calculer la fréquence d’occurrence des 12 classes dans l'image à l'aide de numpy:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "img_carte= img_carte.squeeze() # nécessaire pour ignorer la dimension du canal\n",
        "compte_classe = np.unique(img_carte.data, return_counts=True)\n",
        "print(compte_classe)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "La fréquence d'apparition de chaque classe varie grandement, on parle d'un ensemble déséquilibré. Ceci est très commun dans la plupart des ensembles d’entraînement. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "valeurs, comptes = compte_classe\n",
        "\n",
        "# Create the histogram\n",
        "plt.figure(figsize=(5, 3))\n",
        "plt.bar(valeurs, comptes/comptes.sum()*100)\n",
        "plt.xlabel(\"Classes\")\n",
        "plt.ylabel(\"%\")\n",
        "plt.title(\"Fréquences des classes\")\n",
        "plt.xticks(range(len(nom_classes)), nom_classes, rotation=45, ha='right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "On peut échantillonner 100 points aléatoires pour chaque classe:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "img_carte= img_carte.squeeze()\n",
        "class_counts = np.unique(img_carte.data, return_counts=True)\n",
        "\n",
        "# Liste vide des points échantillonnées\n",
        "sampled_points = []\n",
        "class_labels= []\n",
        "for class_label in range(1,13): # pour chacune des 12 classes\n",
        "  # On cherche tous les pixels pour cette étiquette\n",
        "  class_pixels = np.argwhere(img_carte.data == class_label)\n",
        "\n",
        "  # On se limite à 100 pixels par classe\n",
        "  n_samples = min(100, len(class_pixels))\n",
        "\n",
        "  # On les choisit les positions aléatoirement\n",
        "  np.random.seed(0) # ceci permet de répliquer le tirage aléatoire\n",
        "  sampled_indices = np.random.choice(len(class_pixels), n_samples, replace=False)\n",
        "\n",
        "  # On prends les positions en lignes, colonnes\n",
        "  sampled_pixels = class_pixels[sampled_indices]\n",
        "\n",
        "  # On ajoute les points à la liste\n",
        "  sampled_points.extend(sampled_pixels)\n",
        "  class_labels.extend(np.array([class_label]*n_samples)[:,np.newaxis])\n",
        "\n",
        "# Conversion en NumPy array\n",
        "sampled_points = np.array(sampled_points)\n",
        "class_labels = np.array(class_labels)\n",
        "# On peut naviguer les points à l'aide de la géoréférence\n",
        "transformer = rasterio.transform.AffineTransformer(img_carte.rio.transform())\n",
        "transform_sampled_points= transformer.xy(sampled_points[:,0], sampled_points[:,1])\n",
        "\n",
        "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(8, 6))\n",
        "img_carte.squeeze().plot.imshow(cmap=cmap_classes, vmin=0, vmax=12)\n",
        "ax.scatter(transform_sampled_points[0], transform_sampled_points[1], c='w', s=1)  # Plot sampled points\n",
        "ax.set_title(\"Carte d'occupation des sols avec les points échantillonnés\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Une fois les points sélectionnés, il faut ajouter les valeurs des bandes provenant d'une image satellite. Pour cela, on peut utiliser la méthodes `sample()` de `rasterio`. La librairie [`geopandas`]() permet de gérer les données d’entraînement sous la forme d'un tableau transportant aussi l'information de géoréférence:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "points = [Point(xy) for xy in zip(transform_sampled_points[0], transform_sampled_points[1])]\n",
        "gdf = geopandas.GeoDataFrame(range(1,len(points)+1), geometry=points, crs=img_carte.rio.crs)\n",
        "coord_list = [(x, y) for x, y in zip(gdf[\"geometry\"].x, gdf[\"geometry\"].y)]\n",
        "with rasterio.open('RGBNIR_of_S2A.tif') as src:\n",
        "  gdf[\"value\"] = [x for x in src.sample(coord_list)]\n",
        "gdf['class']= class_labels\n",
        "gdf.to_csv('sampling_points.csv')\n",
        "gdf.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Méthodes non paramétriques\n",
        "\n",
        "### Plus proches voisins\n",
        "\n",
        "La méthode des plus proches voisins (*K-Nearest-Neighbors* en Anglais) est certainement la plus simple des méthodes pour classifier. Elle consiste à comparer une nouvelle données avec ces voisins les plus proches en fonction d'une simple distance Euclidienne. Si une majorité de ces $K$ voisins appartiennent à une classe majoritaire alors cette classe est sélectionnée.\n",
        "\n",
        "Formons un ensemble d’entraînement à partir de notre image RGBNIR précédente:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df= pd.read_csv('sampling_points.csv')\n",
        "# Extraire la colonne 'value'.\n",
        "# 'value' est une chaîne de caractères comme représentation d'une liste de valeurs.\n",
        "# Nous devons la convertir en données numériques réelles.\n",
        "X = df['value'].apply(lambda x: np.fromstring(x[1:-1], dtype=float, sep=' ')).to_list()\n",
        "\n",
        "# on obtient une liste de numpy array  qu'il faut convertir en un numpy array 2D\n",
        "X= np.array([row.tolist() for row in X])\n",
        "idx= X.sum(axis=-1)>0 # il se peut qu'il y ait des valeurs erronées\n",
        "X= X[idx,...]\n",
        "y = df['class'].to_numpy()\n",
        "y= y[idx]\n",
        "class_labels = np.unique(y).tolist() # on cherche à savoir combien de classes uniques\n",
        "n_classes = len(class_labels)\n",
        "if max(class_labels) > n_classes: # il se peut que certaines classes soit absentes\n",
        "  y_new= []\n",
        "  for i,l in enumerate(class_labels):\n",
        "    y_new.extend([i]*sum(y==l))\n",
        "  y_new = np.array(y_new)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Il est important de faire précéder la méthode K-NN par une normalisation des données de façon à ce que chaque caractéristique soit de moyenne 0 et d'écart-type égale à 1 (on dit que l'on blanchit les données). Cette normalisation permet à ce que chaque dimension ait le même poids dans le calcul des distance entre points. Cette opération porte le nom de `StandardScaler` dans `scikit-learn`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "clf = Pipeline(\n",
        "    steps=[(\"scaler\", StandardScaler()), (\"knn\", KNeighborsClassifier(n_neighbors=1))]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Avant d'effectuer un entraînement, on met généralement une portion des données pour valider les performances:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "On peut visualiser l'espace de décision du K-NN pour différentes valeurs de $K$ lorsque seulement deux bandes sont utilisées:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| warning: false\n",
        "\n",
        "_, axs = plt.subplots(nrows=2,ncols=2, figsize=(12, 12))\n",
        "couleurs_classes2= [couleurs_classes[c] for c in np.unique(y).tolist()]\n",
        "cmap_classes2 = ListedColormap(couleurs_classes2)\n",
        "for ax, K in zip(axs.flatten(), [1,3,5,7]):\n",
        "    clf.set_params(knn__weights='distance', knn__n_neighbors = K).fit(X_train[:,2:4], y_train)\n",
        "    disp = DecisionBoundaryDisplay.from_estimator(\n",
        "        clf,\n",
        "        X[:,2:4],\n",
        "        response_method=\"predict\",\n",
        "        plot_method=\"contourf\",\n",
        "        shading=\"auto\",\n",
        "        alpha=0.5,\n",
        "        ax=ax,\n",
        "        cmap= cmap_classes2,\n",
        "    )\n",
        "    \n",
        "    scatter = disp.ax_.scatter(X_test[:, 2], X_test[:, 3], c=y_test, edgecolors=\"k\", cmap=cmap_classes2)\n",
        "    disp.ax_.set_xlabel('R')\n",
        "    disp.ax_.set_ylabel('NIR')\n",
        "    _ = disp.ax_.set_title(\n",
        "        f\"K-NN avec K={clf[-1].n_neighbors}\"\n",
        "    )\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "On peut voir comment les différentes frontières de décision se forme dans l'espace des bandes Rouge-NIR. L'augmentation de K rend ces frontières plus complexes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "clf.set_params(knn__weights='distance', knn__n_neighbors = 7).fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "L'application du modèle (la prédiction) peut se faire sur toute l'image en transposant l'image sous forme d'une matrice avec 4 colonnes:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "data_image= img_rgbnir.to_numpy().transpose(1,2,0).reshape(img_rgbnir.shape[1]*img_rgbnir.shape[2],4)\n",
        "y_classe= clf.predict(data_image)\n",
        "y_classe= y_classe.reshape(img_rgbnir.shape[1],img_rgbnir.shape[2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(8, 6))\n",
        "plt.imshow(y_image, cmap=cmap_classes2)\n",
        "ax.set_title(\"Carte d'occupation des sols avec K-NN\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### SVEM, réseaux de neurones, forêts aléatoires {#sec-0514}\n",
        "\n",
        "### Méthodes par arbre de décision\n",
        "\n",
        "\n",
        "### Réseaux de neurones\n",
        "\n",
        "Les réseaux de neurones artificiels (RNA) ont connu un essor très important depuis les années 2010 avec des approches dites profondes. Ces aspects seront surtout abordés dans le tome 2 consacré à l'intelligence artificielle. On abordera ici seulement le perceptron simple et le perceptron multi-couches (MLP).\n",
        "\n",
        "Le perceptron est l'unité de base d'un RNA et consiste en N connections, une unité de calcul (le neurone) avec une fonction d'activation et une sortie. Le perceptron ne permet de construire que des frontières de décision linéaires.\n",
        "\n",
        "Le perceptron multi-couches est un réseau dense (*fully connected*) avec des couches cachées entre la couche d'entrée et la couche de sortie. qui permet de construire des frontières de décision beaucoup plus complexes via une hiérarchie de frontières de décision.\n",
        "\n",
        "Ces réseaux sont entraînés via des techniques itératives d'optimisation de type descente en gradient avec une correction des paramètres (les poids) à l'aide de la rétro-propagation de l'erreur. L'erreur est mesurée via une fonction de coût que l'on cherche à réduire.\n",
        "\n",
        "\n",
        "\n",
        "## Segmentation d’images {#sec-052}\n",
        "\n",
        "### Classification objet {#sec-0521}\n",
        "\n",
        "### Approches par arbre (BPT, etc.) {#sec-0522}\n",
        "\n",
        "\n",
        "\n",
        "## Exercices de révision {#sec-054}"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}